<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arch on z217&#39;s blog</title>
    <link>https://z217blog.cn/tags/arch/</link>
    <description>Recent content in Arch on z217&#39;s blog</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright © 2020–2026, z217 and the hugo authors, all rights reserved.</copyright>
    <lastBuildDate>Sun, 13 Nov 2022 16:18:06 +0800</lastBuildDate>
    <atom:link href="https://z217blog.cn/tags/arch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>分布式数据系统：共识算法</title>
      <link>https://z217blog.cn/post/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sun, 13 Nov 2022 16:18:06 +0800</pubDate>
      <guid>https://z217blog.cn/post/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/</guid>
      <description>&lt;p&gt;        分布式计算中有很多重要场景需要集群节点达成某种一致，例如：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;主节点选举：对于主从模式的数据库，节点间需要对谁来充当主节点达成一致。如果由于网络故障原因出现节点之间无法通信，就很容易出现争议；&lt;/li&gt;&#xA;&lt;li&gt;原子事务提交：对于支持跨节点或跨分区事务的数据库，某个事务可能在一些节点上执行成功，而在另一些节点上失败。为了维护事务的原子性，所有节点必须对事务结果达成一致。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;1-原子提交与两阶段提交&#34;&gt;1. 原子提交与两阶段提交&lt;/h2&gt;&#xA;&lt;p&gt;        对于单节点事务，原子性通常由存储引擎负责。当客户端请求数据库节点提交事务时，数据库首先使事务的写入持久化 ( 通常保存在&lt;code&gt;WAL&lt;/code&gt;中 )，然后把提交记录追加到磁盘的日志文件中。如果数据库在该过程中发生了崩溃，在节点重启后，可以通过日志恢复事务。如果崩溃之前已经写入了提交记录，则认为事务已经成功，否则，回滚该事务。因此，单节点事务十分依赖于数据写入磁盘的顺序：先写入数据，再提交记录。&lt;br&gt;&#xA;        将单节点事务延伸到多节点，虽然大多数&lt;code&gt;NoSQL&lt;/code&gt;分布式数据库都不支持这种分布式事务，但是有很多集群关系型数据库支持。向所有节点发送请求，然后各节点独立执行是不够的，这样很容易发生不一致，从而违反了原子性。一旦某个节点提交了事务，即使事后发现其他节点发生了中止，它也无法再撤销已提交的事务，所以，如果有部分节点提交了事务，所有节点也必须一起提交。&lt;br&gt;&#xA;        事务提交不可撤销，一旦数据被提交，就代表其他事务可见，继而客户端会依赖这些数据做出相应决策。这是事务&lt;strong&gt;提交读&lt;/strong&gt;隔离级别的基础，如果事务在提交后还能撤销，就违反了提交读的原则，从而被迫产生级联式的追溯和撤销。当然，已提交事务可以被另一个新的事务覆盖，即&lt;em&gt;&lt;strong&gt;补偿性事务&lt;/strong&gt;&lt;/em&gt;。不过，在数据库的角度，它们是两个完全独立的事务，这种跨事务的正确性保证需要应用层负责。&lt;/p&gt;&#xA;&lt;h3 id=&#34;11-两阶段提交&#34;&gt;1.1 两阶段提交&lt;/h3&gt;&#xA;&lt;p&gt;        &lt;strong&gt;两阶段提交&lt;/strong&gt; ( $two-phase\ commit$ , $2PC$ ) 是一种在多节点之间实现事务原子提交的算法，用来确保所有节点要么全部提交，要么全部中止。&lt;code&gt;2PC&lt;/code&gt;在某些数据库内存使用，或者以&lt;code&gt;XA&lt;/code&gt;事务的形式提供给应用程序使用。&lt;br&gt;&#xA;        &lt;code&gt;2PC&lt;/code&gt;引入了单节点事务所没有的新组件：协调者 ( 也被称为事务管理器 )，通常实现为共享库。&lt;code&gt;2PC&lt;/code&gt;事务从应用程序在多个数据库节点上执行数据读/写开始，数据库节点称为事务的参与者。当应用程序准备提交事务时，协调者发送一个准备请求到所有节点，询问它们是否可以进行事务提交：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果所有参与者回答是，表示它们已经准备好提交，协调者会发出提交请求，所有节点开始执行事务提交；&lt;/li&gt;&#xA;&lt;li&gt;如果有任何参与者回答否，协调者会放弃发送提交请求。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;        为了理解&lt;code&gt;2PC&lt;/code&gt;，我们可以分解这个过程：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;应用程序启动一个分布式事务，首先向协调者请求一个全局唯一的事务&lt;code&gt;ID&lt;/code&gt;；&lt;/li&gt;&#xA;&lt;li&gt;应用程序在每个参与节点上执行单节点事务，并将全局唯一事务&lt;code&gt;ID&lt;/code&gt;附加到事务上。此时，每个节点独立执行事务，如果有任何一个节点执行失败，协调者和其他参与者都可以安全回滚事务；&lt;/li&gt;&#xA;&lt;li&gt;应用程序准备提交事务，协调者向所有参与者发送准备请求，附带全局事务&lt;code&gt;ID&lt;/code&gt;。如果接收到拒绝或者超时响应，协调者会通知所有节点放弃事务；&lt;/li&gt;&#xA;&lt;li&gt;参与者在收到准备请求后，检查事务是否可以提交，是否存在冲突或者违反约束。一旦向协调者返回确认响应，无论发生什么情况，都不能拒绝提交事务；&lt;/li&gt;&#xA;&lt;li&gt;协调者收到所有准备请求的响应后，会将决定写入磁盘中，用于崩溃后恢复决定，这个时刻称为提交点；&lt;/li&gt;&#xA;&lt;li&gt;协调者将决定写入磁盘后，向所有参与者发送提交或者放弃请求。如果请求出现失败或者超时，协调者会一种重试，直到成功。所有参与者都不能拒绝该请求，即使需要很多重试，或者中间出现崩溃。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;        如果参与者或者网络在&lt;code&gt;2PC&lt;/code&gt;期间发生故障，比如在准备请求期间，协调者就会决定回滚事务；或者在提交请求期间，协调者会不断重试。而对于协调者故障，如果协调者在准备请求之前故障，参与者可以安全地回滚；而一旦参与者收到了准备请求并回答是，参与者便无法单方面放弃，必须一直等待协调者的决定，此时如果协调者故障，参与者便处于一种不确定的状态。理论上，参与者之间可以互相通信，了解每个参与者的投票情况，并达成一致，但是这已经不是&lt;code&gt;2PC&lt;/code&gt;的范畴了。&lt;code&gt;2PC&lt;/code&gt;能够顺利完成的唯一办法是等待协调者恢复，因此协调者在发送提交请求之前要将决定写入磁盘的事务日志。&lt;br&gt;&#xA;        &lt;code&gt;2PC&lt;/code&gt;也被称为阻塞式原子提交协议，因为等待协调者从故障恢复的这个过程是阻塞的。理论上，也可以改为非阻塞的，这种称为&lt;strong&gt;三阶段提交&lt;/strong&gt;。&lt;code&gt;3PC&lt;/code&gt;假定一个有限的网络延迟，要求节点在规定时间内响应。然而实际情况是，网络延迟可能是无限的。通常，非阻塞原子提交依赖一个完美的故障检测器，即一种十分可靠的可以判断节点是否崩溃的机制。但是，在一个网络延迟可能是无限的场景中，超时并非一种可靠的判断机制。正常情况下，请求也可能由于网络问题而超时。正是这些原因，大家更倾向于&lt;code&gt;2PC&lt;/code&gt;而非&lt;code&gt;3PC&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;h2 id=&#34;2-分布式事务实践&#34;&gt;2. 分布式事务实践&lt;/h2&gt;&#xA;&lt;p&gt;        分布式事务，尤其是那些通过&lt;code&gt;2PC&lt;/code&gt;实现的事务，声誉混杂。一方面，它们提供了一种其他方案难以企及的安全保证。但是另一方面，由于操作、性能上的缺陷，以及并非完全可靠，一直被人诟病。目前，许多云服务商由于运维方面的问题而决定不支持分布式事务。分布式事务的某些实现存在严重的性能问题，例如，有报告显示&lt;code&gt;MySQL&lt;/code&gt;的分布式事务比单节点事务慢 $10$ 倍以上。&lt;code&gt;2PC&lt;/code&gt;性能下降的主要原因是与协调者通信带来额外的网络开销，以及为了协调者崩溃恢复做的磁盘&lt;code&gt;I/O&lt;/code&gt; ( $fsync$ )。&lt;br&gt;&#xA;        目前存在着两种不同的分布式事务概念：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;数据库内部的分布式事务：某些分布式事务支持的跨数据节点的内部事务，即所有参与者节点运行着相同的数据库软件；&lt;/li&gt;&#xA;&lt;li&gt;异构分布式事务：存在两种或两种以上不同参与者软件的事务，例如来自不同供应商的数据库，甚至可以是非数据库。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;        对于数据库内部事务，由于不需要考虑不同系统之间的兼容，可以采用任何形式的协议，并进行针对性优化，这些分布式事务往往可行。但是异构分布式事务就没那么简单了。&lt;/p&gt;&#xA;&lt;h3 id=&#34;21-exactly-once消息处理&#34;&gt;2.1 &lt;code&gt;Exactly-once&lt;/code&gt;消息处理&lt;/h3&gt;&#xA;&lt;p&gt;        异构分布式事务旨在无缝集成多种不同的系统。例如，当且仅当数据库中处理消息的事务成功提交，消息队列才会标记该消息已处理完毕。这个过程是通过自动提交消息确认和数据库写入实现的。即使消息系统和数据库运行在不同节点上，分布式事务也能实现上述目标。如果消息发送失败或者某个节点事务失败，两者都必须中止。消息队列可以在之后重传消息。因此通过自动提交和消息处理结果，可以确保消息有效处理只有一次。&lt;br&gt;&#xA;        需要注意，只有所有相关系统都使用相同的原子性提交协议的前提下，这种分布式事务才是可行的。例如，如果处理结果之一是发送邮件，而邮件服务器不支持&lt;code&gt;2PC&lt;/code&gt;，此时某个过程出错，消息重新入队重试，邮件就可能会被发送多次。&lt;/p&gt;&#xA;&lt;h3 id=&#34;22-xa事务&#34;&gt;2.2 &lt;code&gt;XA&lt;/code&gt;事务&lt;/h3&gt;&#xA;&lt;p&gt;        &lt;code&gt;X/Open XA&lt;/code&gt; ( $eXtended\ Architecture$ , $XA$ ) 是异构环境下进行&lt;code&gt;2PC&lt;/code&gt;的一个工业标准。目前，许多关系型数据库 ( &lt;code&gt;PostgreSQL&lt;/code&gt;、&lt;code&gt;MySQL&lt;/code&gt;、&lt;code&gt;Oracle&lt;/code&gt;等 ) 和消息队列 ( &lt;code&gt;ActiveMQ&lt;/code&gt;、&lt;code&gt;MSMQ&lt;/code&gt;、&lt;code&gt;IBM MQ&lt;/code&gt;等 ) 都支持&lt;code&gt;XA&lt;/code&gt;。&lt;code&gt;XA&lt;/code&gt;并不是一个网络协议，而是一个与事务协调者进行通信的&lt;code&gt;C API&lt;/code&gt;。当然，它也支持与其他语言的&lt;code&gt;API&lt;/code&gt;绑定，例如&lt;code&gt;Java&lt;/code&gt;。&lt;br&gt;&#xA;        &lt;code&gt;XA&lt;/code&gt;假定应用程序通过网络或客户端的库函数与参与者节点进行通信，如果驱动程序支持&lt;code&gt;XA&lt;/code&gt;，意味着应用程序可以调用&lt;code&gt;XA API&lt;/code&gt;确定操作是否属于异构分布式事务的一部分。如果是，则发送必要的信息给数据库服务器。它还支持回调，这样协调者可以通过回调函数通知所有参与者执行准备或者提交 ( 或者中止 )。&lt;br&gt;&#xA;        协调者需要实现&lt;code&gt;XA API&lt;/code&gt;。虽然标准没有规定如何实现，但实际上，协调者也通常是一个&lt;code&gt;API&lt;/code&gt;库，与产生事务的应用程序运行在相同进程中。这些&lt;code&gt;API&lt;/code&gt;跟踪事务的参与者，收集投票，并在本地磁盘中记录决定。如果应用程序发生崩溃，或者节点故障，在重启后，协调者会通过&lt;code&gt;XA API&lt;/code&gt;读取日志，恢复决定。完成这些后，协调者才能继续通过回调函数来要求参与者执行提交或者中止。数据库服务器无法直接与协调者通信，必须通过相应&lt;code&gt;API&lt;/code&gt;。&lt;/p&gt;</description>
    </item>
    <item>
      <title>分布式数据系统：主从节点</title>
      <link>https://z217blog.cn/post/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E4%B8%BB%E4%BB%8E%E8%8A%82%E7%82%B9/</link>
      <pubDate>Sun, 06 Nov 2022 15:24:37 +0800</pubDate>
      <guid>https://z217blog.cn/post/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E4%B8%BB%E4%BB%8E%E8%8A%82%E7%82%B9/</guid>
      <description>&lt;h2 id=&#34;1-主从模式&#34;&gt;1. 主从模式&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-同步复制&#34;&gt;1.1 同步复制&lt;/h3&gt;&#xA;&lt;p&gt;        对于关系型数据库，同步或者异步通常是一个可选项，而其他系统可能是硬性指定二选一。&lt;br&gt;&#xA;        同步复制的优点是：一旦向用户确认，从节点可以保证已经完成了与主节点的同步，数据已经处于最新版本。如果主节点发生故障，总是可以确保之后继续访问从节点的数据一定是最新的。缺点是：如果同步的从节点无法确认成功，整个写入就不能成功，主节点会阻塞之后的所有写操作，直到同步副本确认。由于该缺点的存在，把所有从节点都配置为同步复制有些不切实际。在实践中，如果数据库启用了同步复制，通常意味着其中某个从节点是同步的，其他从节点则是异步的。如果同步的从节点不可用或者性能下降，则将另一个从节点升级为同步。这样可以保证至少有两个节点拥有最新的数据，这种配置也被称为&lt;strong&gt;半同步&lt;/strong&gt;。&lt;br&gt;&#xA;        在主从模式下，如果要提高读性能，需要添加更多的从节点。但是，这种方式实际上只能用于异步模式，因为随着从节点的增加，全同步模式需要同步的从节点数量也会增加，任何单节点的故障或者网络中断都会导致整个集群无法写入，节点的增加也会提高故障机率，所以全同步模式在实践中是非常不可靠的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;12-异步复制&#34;&gt;1.2 异步复制&lt;/h3&gt;&#xA;&lt;p&gt;        主从复制通常会被配置为全异步模式。此时，如果主节点失败且不可恢复，则所有尚未同步的从节点的写请求都会丢失，即使已经向客户端确认了写操作完成，仍然无法保证数据的持久化。全异步模式的优点是，不管从节点的数据多么滞后，总是可以继续响应写请求，具有较高吞吐性。异步模式听起来很不靠谱，但是却被广泛使用，特别是那些从节点数据巨大，或者分布于广域地理环境的情况。&lt;br&gt;&#xA;        异步模式下节点的数据同步可能存在滞后，意味着对主节点和从节点同时发起相同的查询，返回的结果可能是不一致的。但是，这种不一致只是暂时的状态，在不写数据库的情况下，从节点最终会与主节点的数据保持一致，这种效应也被称为&lt;strong&gt;最终一致性&lt;/strong&gt;。&lt;br&gt;&#xA;        理论上，复制的滞后并没有上限。正常情况下，这个延迟可能不到 $1s$ ，实践中通常不会有太大影响。但是，如果系统的性能抵达上限，或者存在网络问题，延迟可能会达到几秒甚至几分钟。&lt;/p&gt;&#xA;&lt;h4 id=&#34;121-读写一致性&#34;&gt;1.2.1 读写一致性&lt;/h4&gt;&#xA;&lt;p&gt;        许多应用让用户提交数据，并在之后查询这些数据。用户向主节点提交数据后，之后的查询可能是在从节点上，在大多数情况下，这是个很合适的方案。然而，异步模式下，同步可能存在滞后，意味着，返回的数据是旧数据，在用户看来，代表他刚刚提交的数据丢了。&lt;br&gt;&#xA;        这种情况，我们需要&lt;strong&gt;读写一致性&lt;/strong&gt;，或者叫&lt;em&gt;&lt;strong&gt;写后读一致性&lt;/strong&gt;&lt;/em&gt;。该机制保证，如果用户重新加载页面，总是能看到最近提交的更新。有几种方式可以实现读写一致性：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果用户访问可能会被修改的内容，从主节点读取。这种方式要球一些方法在执行实际查询前，就知道内容是否改变。比如，社交网络上的用户信息通常只能由所有者编辑，因此，可以让用户总是从主节点读取自己的用户信息，从节点读其他人的用户信息；&lt;/li&gt;&#xA;&lt;li&gt;如果应用的大部分内容都可以被所有用户修改，这种方式就不太有效了。此时需要其他方式来判断，比如跟踪更新时间，在更新后的一分钟内，总是从主节点读取；或者监控从节点的复制滞后程度，避免从滞后超过一分钟的从节点读取数据；&lt;/li&gt;&#xA;&lt;li&gt;客户端记录最近更新的时间戳，附带在请求中，节点可以通过该时间戳确保返回该时间戳之后的更新，如果无法返回，交由另外一个节点处理。时间戳可以是逻辑时间戳 ( 比如日志序列号 ) 或者实际系统时间；&lt;/li&gt;&#xA;&lt;li&gt;如果副本分布在多个数据中心，这时情况会比较复杂，应当将请求路由到主节点所在的数据中心，即使该数据中心可能离用户距离很远。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;        如果同一用户从多端访问 ( 比如&lt;code&gt;Web&lt;/code&gt;端和移动端 )，情况就更加复杂了，不仅要提供跨设备的读写一致性，还有新问题：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;时间戳方式难以实现，因为一台设备并不知道另一台设备的操作，如果需要实现时间戳方式，元数据需要做到全局共享；&lt;/li&gt;&#xA;&lt;li&gt;如果副本分布在多数据中心，无法保证来自不同设备的连接路由到同一个数据中心。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;122-单调读&#34;&gt;1.2.2 单调读&lt;/h4&gt;&#xA;&lt;p&gt;        假定用户发起多次读取，读请求可能会被路由到不同节点，则可能会出现请求返回不同结果的情况。&lt;strong&gt;单调读&lt;/strong&gt;一致性可以确保不会发生这种异常。单调读一致性比强一致性弱，但是比最终一致性强。读取数据时，单调读保证同一个用户一次发起的多次读取不会看到回滚 ( 数据不一致 ) 现象。一种实现单调读的方式是：确保每个用户总是固定读同一个节点，例如基于用户&lt;code&gt;ID&lt;/code&gt;哈希。&lt;/p&gt;&#xA;&lt;h4 id=&#34;123-前缀一致读&#34;&gt;1.2.3 前缀一致读&lt;/h4&gt;&#xA;&lt;p&gt;        用户发起两个请求，后一个请求的内容依赖于前一个请求，比如用户先写入 $1$ ，再递增为 $2$ 。从用户的角度，这个顺序没有问题。但是在其他观察者的角度，可能会存在逻辑问题，比如由于网络延迟，观察者先观察到后一个请求，这时顺序就变成了用户先写入 $2$ ，再写入 $1$ 。为了防止这种异常，需要引入&lt;strong&gt;前缀一致性&lt;/strong&gt;，即对于一系列按照某个顺序发起的写请求，读取的时候也应该按照这个顺序。&lt;br&gt;&#xA;        这个问题是存在于分区数据库的一个特殊问题。如果数据库总是以相同的顺序写入，那么读取的时候看到的会是一致的序列。但是，分区数据库的不同分区之间是独立运行的，所以没有一个全局的写入顺序，导致用户读取的时候，会读到一部分新值和一部分旧值。一种解决方案是：将所有具有因果关系的写入都交给同一个分区完成，但是会导致效率大打折扣。&lt;/p&gt;&#xA;&lt;h3 id=&#34;13-节点失效&#34;&gt;1.3 节点失效&lt;/h3&gt;&#xA;&lt;h4 id=&#34;131-从节点失效&#34;&gt;1.3.1 从节点失效&lt;/h4&gt;&#xA;&lt;p&gt;        从节点的磁盘上保存了数据变更日志，如果从节点崩溃，或者与主节点之间发生暂时性的网络中断，可以通过该日志获取故障前处理的最后一个事务，向主节点请求该事务之后的中断期内所有数据变更，将其应用到本地即可，之后就可以像正常情况一样持续接收来自主节点的数据流变化。&lt;/p&gt;&#xA;&lt;h4 id=&#34;132-主节点失效&#34;&gt;1.3.2 主节点失效&lt;/h4&gt;&#xA;&lt;p&gt;        主节点失效，则需要选择某个从节点，将其提升为主节点。同时，客户端也要将之后的写请求发送给新的主节点。故障切换可以是手动的，也可以是自动的，自动切换的步骤如下：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;确认主节点失效。主节点可能出于多种原因失效，比如系统崩溃、停电、网络中断等，并没有什么办法可以检测出失效原因，所以大多数系统都采用了超时机制判断。节点间会持续地互相发送心跳消息，如果发现某个节点在一段时间 ( 比如 $30s$ ) 内都没有响应，就认为该节点已经失效；&lt;/li&gt;&#xA;&lt;li&gt;选举新的主节点。新的主节点可以通过一种共识算法来选举，或者由控制节点指定。候选节点最好是与主节点之间数据差异最小的，从而最小化数据丢失的风险；&lt;/li&gt;&#xA;&lt;li&gt;重新配置系统使得新主节点生效。客户端需要将写请求发送给新的主节点，如果原来的主节点之后重新上线，需要将其降级为从节点，并认可新的主节点。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;        切换过程中可能存在很多变数：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果使用异步复制，新的主节点相比原主节点的数据存在滞后，在选举完成后，原主节点又很快恢复并加入集群，接下来的写操作要怎么处理？新的主节点可能会收到冲突的写请求，因为原主节点并没有意识到发生了主节点切换，仍然会尝试同步其他从节点。一种常见的解决方案是，直接丢弃这些冲突的写请求，虽然这会违背数据更新持久化的承诺；&lt;/li&gt;&#xA;&lt;li&gt;如果有其他依赖于数据库的组件在一起协同使用，丢弃数据的方案就很危险。例如，在&lt;code&gt;GitHub&lt;/code&gt;的一个事故中，主节点在未完全同步的情况下失效，新主节点被选举，由于存在滞后，原主节点已经分配出去，存储在&lt;code&gt;Redis&lt;/code&gt;中的自增主键，被新主节点再次使用，导致了&lt;code&gt;MySQL&lt;/code&gt;与&lt;code&gt;Redis&lt;/code&gt;之间的数据不一致；&lt;/li&gt;&#xA;&lt;li&gt;某些情况下，可能会发生两个节点都认为自己是主节点的情况，称为&lt;strong&gt;脑裂&lt;/strong&gt;。脑裂非常危险，它可能会导致两个主节点同时接收写请求，并且没有很好的解决数据冲突，导致数据被丢失或者被破坏。有些系统会通过强制关闭其中一个节点的方式，来解决脑裂问题。然而，如果设计或者实现考虑不周，也是有可能出现两个节点都被关闭的情况；&lt;/li&gt;&#xA;&lt;li&gt;如果设置超时时间？超时时间越厂，确认主节点失效的时间也就越长，意味着总体的恢复时间就越长。例如，突发的负载峰值会导致节点的响应时间变长甚至超时，或者由于网络故障导致延迟增加，如果这时系统已经处于高负载情况，或者网络严重拥堵的情况，不必要的切换只会使情况更糟。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;14-日志&#34;&gt;1.4 日志&lt;/h3&gt;&#xA;&lt;h4 id=&#34;141-语句复制&#34;&gt;1.4.1 语句复制&lt;/h4&gt;&#xA;&lt;p&gt;        主节点记录执行的写请求语句，将该操作语句作为日志，发送给从节点。在关系型数据库中，意味着发送 $INSERT$ 、$UPDATE$ 等语句，从节点分析并执行这些语句，像来自客户端那样。基于语句复制存在一些不适用场景：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
